{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Type, Union, List\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Pipeline\n",
    "1. Read formatted data as DF\n",
    "2. Add salary tiers to DF\n",
    "3. Retain desired subset of features and remove rest from DF\n",
    "4. Split data into train/test\n",
    "5. Determine transformations for data (numerical/categorical)\n",
    "6. Map numerical/categorical transformations to appropriate features using ColumnTransformer\n",
    "7. Add transformations to a sklearn Pipeline\n",
    "8. Add a predictive model to the same Pipeline\n",
    "9. Fit model on training data\n",
    "10. Cross-validate\n",
    "11. Score on test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "debug = True\n",
    "src_dir = os.getcwd()\n",
    "data_dir = os.path.join(src_dir, \"Data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_data_file = os.path.join(data_dir, \"nba_stats_sal_merged_1990_2017.csv\")\n",
    "merged_data = pd.read_csv(merged_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data:\n",
      "       Season End             Player Pos  Age   Tm  BLK  TOV   PF   PTS  \\\n",
      "0            1990         Mark Acres   C   27  ORL   25   70  248   362   \n",
      "1            1990      Michael Adams  PG   27  DEN    3  141  133  1221   \n",
      "2            1990       Mark Aguirre  SF   30  DET   19  121  201  1099   \n",
      "3            1990        Danny Ainge  PG   30  SAC   18  185  238  1342   \n",
      "4            1990        Mark Alarie  PF   26  WSB   39  101  219   860   \n",
      "10639        2017        Cody Zeller  PF   24  CHO   58   65  189   639   \n",
      "10640        2017       Tyler Zeller   C   27  BOS   21   20   61   178   \n",
      "10641        2017  Stephen Zimmerman   C   20  ORL    5    3   17    23   \n",
      "10642        2017        Paul Zipser  SF   22  CHI   16   40   78   240   \n",
      "10643        2017        Ivica Zubac   C   19  LAL   33   30   66   284   \n",
      "\n",
      "         Salary  \n",
      "0        437000  \n",
      "1        825000  \n",
      "2       1115000  \n",
      "3        725000  \n",
      "4        500000  \n",
      "10639  12584270  \n",
      "10640   1709538  \n",
      "10641   1312611  \n",
      "10642   1312611  \n",
      "10643   1312611  \n"
     ]
    }
   ],
   "source": [
    "if debug: \n",
    "    print(\"Input Data:\")\n",
    "    print(merged_data.iloc[list(range(5)) + list(range(-5, 0)), \n",
    "                   list(range(5)) + list(range(-5, 0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Season End      int64\n",
       "Player         object\n",
       "Pos            object\n",
       "Age             int64\n",
       "Tm             object\n",
       "G               int64\n",
       "GS              int64\n",
       "MP              int64\n",
       "PER           float64\n",
       "TS%           float64\n",
       "3PAr          float64\n",
       "FTr           float64\n",
       "ORB%          float64\n",
       "DRB%          float64\n",
       "TRB%          float64\n",
       "AST%          float64\n",
       "STL%          float64\n",
       "BLK%          float64\n",
       "TOV%          float64\n",
       "USG%          float64\n",
       "OWS           float64\n",
       "DWS           float64\n",
       "WS            float64\n",
       "WS/48         float64\n",
       "OBPM          float64\n",
       "DBPM          float64\n",
       "BPM           float64\n",
       "VORP          float64\n",
       "FG              int64\n",
       "FGA             int64\n",
       "FG%           float64\n",
       "3P              int64\n",
       "3PA             int64\n",
       "3P%           float64\n",
       "2P              int64\n",
       "2PA             int64\n",
       "2P%           float64\n",
       "eFG%          float64\n",
       "FT              int64\n",
       "FTA             int64\n",
       "FT%           float64\n",
       "ORB             int64\n",
       "DRB             int64\n",
       "TRB             int64\n",
       "AST             int64\n",
       "STL             int64\n",
       "BLK             int64\n",
       "TOV             int64\n",
       "PF              int64\n",
       "PTS             int64\n",
       "Salary          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Steps\n",
    "- Already took care of null data (when reading from original CSVs and merging)\n",
    "- Try different normalization, standardization techniques and see effect on performance\n",
    "- The only categorical variable to be encoded is Position; not using Team unfortunately b/c a) teams have changed and b) some players played for multiple teams, and thus have TOT as their team. Note that this variable is not ordinal, it's nominal. Also, beware multicollinearity.\n",
    "- Split data by year, do scaling on separate DataFrames, then concatenate.\n",
    "- Write different scaled data to different CSVs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Salary Tiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_tiers = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_salary_tiers(num_tiers: int, merged_data: pd.DataFrame):\n",
    "    # Compute max salaries per year\n",
    "    salary_maxes = merged_data.groupby('Season End')['Salary'].max().to_dict()\n",
    "    # Append salary tiers per player in a new column\n",
    "    player_tiers = []\n",
    "    for index, row in merged_data[['Season End', 'Salary']].iterrows():\n",
    "        # +1 so we have num_tiers tiers (so the max falls in the highest tier)\n",
    "        player_tiers.append(int(row['Salary'] / (salary_maxes[row['Season End']]+1) * num_tiers))\n",
    "    if not 'Salary Tier' in merged_data.columns: \n",
    "        merged_data.insert(len(merged_data.columns), 'Salary Tier', pd.Series(player_tiers))\n",
    "    else:\n",
    "        merged_data['Salary Tier'] = pd.Series(player_tiers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged With Salary Tier:\n",
      "       Season End             Player Pos  Age   Tm  TOV   PF   PTS    Salary  \\\n",
      "0            1990         Mark Acres   C   27  ORL   70  248   362    437000   \n",
      "1            1990      Michael Adams  PG   27  DEN  141  133  1221    825000   \n",
      "2            1990       Mark Aguirre  SF   30  DET  121  201  1099   1115000   \n",
      "3            1990        Danny Ainge  PG   30  SAC  185  238  1342    725000   \n",
      "4            1990        Mark Alarie  PF   26  WSB  101  219   860    500000   \n",
      "10639        2017        Cody Zeller  PF   24  CHO   65  189   639  12584270   \n",
      "10640        2017       Tyler Zeller   C   27  BOS   20   61   178   1709538   \n",
      "10641        2017  Stephen Zimmerman   C   20  ORL    3   17    23   1312611   \n",
      "10642        2017        Paul Zipser  SF   22  CHI   40   78   240   1312611   \n",
      "10643        2017        Ivica Zubac   C   19  LAL   30   66   284   1312611   \n",
      "\n",
      "       Salary Tier  \n",
      "0                0  \n",
      "1                0  \n",
      "2                1  \n",
      "3                0  \n",
      "4                0  \n",
      "10639            1  \n",
      "10640            0  \n",
      "10641            0  \n",
      "10642            0  \n",
      "10643            0  \n"
     ]
    }
   ],
   "source": [
    "add_salary_tiers(num_tiers, merged_data)\n",
    "if debug: \n",
    "    print(\"Merged With Salary Tier:\")\n",
    "    print(merged_data.iloc[list(range(5)) + list(range(-5, 0)), \n",
    "                   list(range(5)) + list(range(-5, 0))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_salary_info(merged_sal_tier: pd.DataFrame):\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "    fig.set_size_inches(15, 5)\n",
    "    merged_data['Salary Tier'].plot.hist(ax=ax1, bins=num_tiers, xticks=[0, 1, 2, 3, 4])\n",
    "    ax1.set_xlabel(\"Salary Tiers\")\n",
    "    merged_data.groupby('Season End')['Salary'].mean().sort_index().plot.bar(ax=ax2)\n",
    "    ax2.set_xlabel(\"Year\")\n",
    "    ax2.set_ylabel(\"Mean Salary\")\n",
    "    plt.show()\n",
    "\n",
    "if debug: plot_salary_info(merged_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scikit-learn Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 80/20 Train-Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged w_sal columns:\n",
      " Index(['Season End', 'Player', 'Pos', 'Age', 'Tm', 'G', 'GS', 'MP', 'PER',\n",
      "       'TS%', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TRB%', 'AST%', 'STL%', 'BLK%',\n",
      "       'TOV%', 'USG%', 'OWS', 'DWS', 'WS', 'WS/48', 'OBPM', 'DBPM', 'BPM',\n",
      "       'VORP', 'FG', 'FGA', 'FG%', '3P', '3PA', '3P%', '2P', '2PA', '2P%',\n",
      "       'eFG%', 'FT', 'FTA', 'FT%', 'ORB', 'DRB', 'TRB', 'AST', 'STL', 'BLK',\n",
      "       'TOV', 'PF', 'PTS', 'Salary', 'Salary Tier'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "if debug: print(\"Merged w_sal columns:\\n\", merged_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#features to omit\n",
    "def construct_final_df(merged_w_sal: pd.DataFrame, to_omit: List[str]):\n",
    "    # Check that to_omit contains valid features\n",
    "    if all(feature in merged_w_sal.columns for feature in to_omit):\n",
    "        return merged_w_sal[[feature for feature in merged_w_sal.columns if not feature in to_omit]]\n",
    "    else:\n",
    "        raise ValueError(\"to_omit must contain valid features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (8515, 48) | y_train shape: (8515,)\n",
      "X_test shape: (2129, 48) | y_test shape: (2129,)\n"
     ]
    }
   ],
   "source": [
    "merged_data_final = construct_final_df(merged_data, ['Player', 'Tm', 'Salary'])\n",
    "#merged_data_final = construct_final_df(merged_data_final, \n",
    "#                                       ['Pos', 'G', 'GS', 'MP', '3PAr', 'FTr', 'ORB%', 'DRB%', 'TOV%', 'WS/48',\n",
    "#                                       'OBPM', 'DBPM', 'VORP', 'FG', 'FGA', '3P', '3PA', '3P%', '2P', '2PA', '2P%',\n",
    "#                                       'FTA', 'ORB', 'DRB', 'PF'])\n",
    "#these two lines always in supervisedlearning, data in y, \n",
    "y = merged_data_final.pop('Salary Tier')\n",
    "#train fiiting, test scoring\n",
    "X_train, X_test, y_train, y_test = train_test_split(merged_data_final, y, test_size=0.2)\n",
    "if debug: \n",
    "    print(\"X_train shape: {} | y_train shape: {}\".format(X_train.shape, y_train.shape))\n",
    "    print(\"X_test shape: {} | y_test shape: {}\".format(X_test.shape, y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encoding of Pos and Multicollinearity\n",
    "We see that we don't need to worry about multicollinearity from these values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PF       1821\n",
      "C        1751\n",
      "PG       1649\n",
      "SG       1635\n",
      "SF       1537\n",
      "SG-SF      24\n",
      "SF-SG      19\n",
      "PG-SG      18\n",
      "PF-C       16\n",
      "SG-PG      13\n",
      "PF-SF      10\n",
      "C-PF       10\n",
      "SF-PF       8\n",
      "SG-PF       3\n",
      "PG-SF       1\n",
      "Name: Pos, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "if debug: \n",
    "    if 'Pos' in X_train.columns:\n",
    "        print(X_train['Pos'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For reference, note that Transformers don't like 1D numpy arrays\n",
    "#ohe = OneHotEncoder(sparse=False)\n",
    "#print(ohe.fit_transform(X_train['Pos'].values.reshape(-1, 1)).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical Transformation Pipeline\n",
    "We will use a ColumnTransformer to fit transformations in parallel rather than run `cat_pipe.fit_transform(cat_Xtrain)` and `num_pipe.fit_transform(num_Xtrain)` ourselves sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TransformerMixin: Need to implem fit and transform, we get fit_transform which invokes both\n",
    "# BaseEstimator: We get get_params and set_params\n",
    "class YearlyScaler(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Performs a Scaler Transform on the input DataFrame by year (groupby split-apply-combine strategy).\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    year_col : The name of the column containing years (to use for splitting)\n",
    "    scaler : Some kind of Transformer\n",
    "    return_df : Return a pd.Dataframe if True\n",
    "    \"\"\"\n",
    "    def __init__(self, year_col: str, scaler: Type[Union[BaseEstimator, TransformerMixin]], \n",
    "                 return_df: bool = False):\n",
    "        # TODO: FOR SOME FUCKING REASON THESE CAN'T BE MANGLED...\n",
    "        self.scaler = scaler\n",
    "        self.year_col = year_col\n",
    "        self.return_df = return_df\n",
    "    \n",
    "    def fit(self, X: pd.DataFrame, y=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame, shape [n_samples, n_features]\n",
    "            The data to fit.\n",
    "        y : Ignored\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self: object\n",
    "            Fitted scaler\n",
    "        \"\"\"\n",
    "        # TODO: ...BUT THESE CAN...\n",
    "        # Omit the groupby column\n",
    "        self._columns = [col for col in X.columns.values if col != self.year_col]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> Union[pd.DataFrame, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : DataFrame, shape [n_samples, n_features]\n",
    "            The data used to scale along the features axis.\n",
    "            \n",
    "        Returns\n",
    "        -------\n",
    "        X_tr : array-like, shape [n_samples, n_features]\n",
    "            Transformed array.\n",
    "        \"\"\"\n",
    "        # Apply the self._scaler transform on all the numerical columns grouped by the Year column\n",
    "        # Remove the scaled Year column (iloc[:, 1:] says all rows, ignore 0th column, which is Year)\n",
    "        # Will give us a multi-indexed DF with the years as the 0-level index\n",
    "        X_tr = X[[self.year_col]+self._columns].groupby(self.year_col).apply(\n",
    "            lambda x, scaler=self.scaler: pd.DataFrame(scaler.fit_transform(x)).iloc[:, 1:])\n",
    "        # Drop the year index\n",
    "        X_tr.index = X_tr.index.droplevel(0)\n",
    "        \n",
    "        return X_tr if self.return_df else X_tr.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_transformer(X_train: pd.DataFrame, \n",
    "                          scaler: Type[Union[BaseEstimator, TransformerMixin]],\n",
    "                          year_col: str, n_jobs: int = None) -> ColumnTransformer:\n",
    "    \"\"\"\n",
    "    n_jobs: -1 means use all processors. None means use 1.\n",
    "    \"\"\"\n",
    "    # Get column types(help determine if categorical)\n",
    "    kinds = np.array([dt.kind for dt in X_train.dtypes])\n",
    "    all_columns = X_train.columns.values\n",
    "    #numpy stuff, create mask\n",
    "    is_num = kinds != 'O'\n",
    "    num_cols = all_columns[is_num]\n",
    "    cat_cols = all_columns[~is_num]\n",
    "\n",
    "    # Define labeled categorical transformation steps\n",
    "    # Dense matrix for potential time/space complexity\n",
    "    # When encountering unknown labels (for test transform), ignore them\n",
    "    cat_ohe_step = ('ohe', OneHotEncoder(sparse=False, handle_unknown='ignore')) \n",
    "\n",
    "    # Put the categorical transformations in a Pipeline to be executed sequentially\n",
    "    cat_steps = [cat_ohe_step]\n",
    "    cat_pipe = Pipeline(cat_steps)\n",
    "\n",
    "    # Define labeled numerical transformation steps\n",
    "    num_scale_step = ('scale', YearlyScaler(year_col, scaler(), return_df=True))\n",
    "\n",
    "    # Would do the following if creating a Pipeline to feed to ColumnTransformer\n",
    "    # Put the numerical transformations in a Pipeline also\n",
    "    num_steps = [num_scale_step]\n",
    "    num_pipe = Pipeline(steps=num_steps)\n",
    "\n",
    "    cat_col_transformers = ('cat', cat_pipe, cat_cols)\n",
    "    num_col_transformers = ('num', num_pipe, num_cols)\n",
    "\n",
    "    by_col_transformers = [cat_col_transformers, num_col_transformers]\n",
    "\n",
    "    # Create ColumnTransformer which will apply transformations in parallel where possible\n",
    "    col_transformer = ColumnTransformer(transformers=by_col_transformers, n_jobs=n_jobs)\n",
    "    # We pass the whole training DataFrame because the transformers know which Pipelines\n",
    "    # to apply to which columns (specified in the 'transformers' list)\n",
    "    \n",
    "    return col_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make the following a new notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformed shape: (8515, 61)\n",
      "       10   11   12   13   14        15        16        17        18  \\\n",
      "0     0.0  1.0  0.0  0.0  0.0  0.400000  0.222222  0.623188  0.648448   \n",
      "1     0.0  0.0  0.0  0.0  0.0 -0.200000  0.166667  0.086957  0.056005   \n",
      "2     0.0  0.0  0.0  0.0  0.0 -0.600000 -1.444444 -0.507246 -1.031714   \n",
      "3     0.0  0.0  0.0  0.0  0.0 -0.400000  0.222222 -0.144928 -0.009447   \n",
      "4     0.0  1.0  0.0  0.0  0.0  0.400000 -0.111111  0.565217  0.477058   \n",
      "5     0.0  1.0  0.0  0.0  0.0 -0.800000  0.222222 -0.507246 -0.383941   \n",
      "6     0.0  0.0  0.0  0.0  0.0  0.000000  0.277778  0.681159  0.851552   \n",
      "7     0.0  0.0  0.0  1.0  0.0  1.000000 -0.111111  0.449275  0.277328   \n",
      "8     0.0  0.0  0.0  0.0  0.0 -0.200000  0.277778  0.492754  0.322537   \n",
      "9     0.0  0.0  0.0  0.0  0.0  0.000000  0.277778  0.347826  0.427126   \n",
      "10    0.0  0.0  0.0  0.0  0.0  0.400000  0.277778  0.115942  0.348853   \n",
      "11    0.0  0.0  0.0  0.0  1.0 -0.200000 -3.722222 -0.478261 -1.163968   \n",
      "12    0.0  1.0  0.0  0.0  0.0  0.600000  0.277778  0.449275  0.253036   \n",
      "13    0.0  1.0  0.0  0.0  0.0  0.200000  0.222222  0.231884  0.278677   \n",
      "14    0.0  0.0  0.0  0.0  0.0 -0.200000  0.111111  0.637681  0.726721   \n",
      "15    0.0  0.0  0.0  0.0  0.0  0.000000  0.277778  0.652174  0.505398   \n",
      "16    0.0  0.0  0.0  0.0  0.0  0.800000  0.222222 -0.478261 -0.091093   \n",
      "17    0.0  0.0  0.0  0.0  0.0 -0.800000 -0.277778 -0.434783 -0.431174   \n",
      "18    0.0  0.0  0.0  0.0  0.0 -0.200000 -0.388889  0.420290  0.319838   \n",
      "19    0.0  1.0  0.0  0.0  0.0  0.600000  0.277778  0.681159  0.634953   \n",
      "20    0.0  0.0  0.0  0.0  0.0  1.200000 -0.055556  0.521739  0.330634   \n",
      "21    0.0  0.0  0.0  0.0  0.0  0.600000  0.277778  0.637681  0.363698   \n",
      "22    0.0  0.0  0.0  0.0  0.0 -0.200000 -0.111111 -0.463768 -0.111336   \n",
      "23    1.0  0.0  0.0  0.0  0.0  0.000000 -0.555556 -0.130435 -0.523617   \n",
      "24    0.0  1.0  0.0  0.0  0.0  2.000000  0.111111  0.623188  0.361673   \n",
      "25    0.0  1.0  0.0  0.0  0.0 -0.200000 -0.388889 -0.043478 -0.269906   \n",
      "26    0.0  1.0  0.0  0.0  0.0 -0.600000 -0.166667  0.565217  0.622132   \n",
      "27    0.0  0.0  0.0  0.0  0.0  0.200000 -1.555556 -0.507246 -1.047908   \n",
      "28    0.0  0.0  0.0  0.0  0.0 -0.600000  0.111111  0.623188  0.541835   \n",
      "29    0.0  0.0  0.0  0.0  0.0  1.000000  0.166667 -0.463768 -0.355601   \n",
      "...   ...  ...  ...  ...  ...       ...       ...       ...       ...   \n",
      "8485  0.0  0.0  0.0  0.0  0.0 -1.000000  0.428571  1.239437  0.811399   \n",
      "8486  0.0  0.0  0.0  0.0  0.0  0.333333  0.257143  1.126761  0.591019   \n",
      "8487  0.0  0.0  0.0  0.0  0.0 -0.833333 -1.628571 -0.281690 -0.916408   \n",
      "8488  0.0  0.0  0.0  0.0  0.0 -0.500000  0.057143 -0.169014 -0.215199   \n",
      "8489  0.0  1.0  0.0  0.0  0.0 -0.833333 -0.028571 -0.244131 -0.280829   \n",
      "8490  0.0  0.0  0.0  0.0  0.0 -1.000000 -0.314286  0.488263  0.027288   \n",
      "8491  0.0  1.0  0.0  0.0  0.0 -0.333333  0.428571  0.676056  0.529534   \n",
      "8492  0.0  0.0  0.0  0.0  0.0 -0.166667 -1.742857 -0.281690 -0.901209   \n",
      "8493  0.0  1.0  0.0  0.0  0.0  1.500000  0.342857 -0.018779  0.329188   \n",
      "8494  0.0  0.0  0.0  0.0  0.0 -0.500000 -0.200000  0.056338 -0.022453   \n",
      "8495  0.0  1.0  0.0  0.0  0.0  0.333333  0.428571  1.239437  0.991710   \n",
      "8496  0.0  0.0  0.0  0.0  0.0  1.500000  0.200000 -0.187793  0.289810   \n",
      "8497  0.0  0.0  0.0  0.0  0.0 -0.333333  0.428571  1.239437  0.951641   \n",
      "8498  0.0  1.0  0.0  0.0  0.0  0.333333 -0.114286  0.882629  0.486010   \n",
      "8499  0.0  0.0  0.0  0.0  0.0 -1.000000 -1.742857 -0.262911 -0.908117   \n",
      "8500  0.0  0.0  0.0  0.0  0.0  0.333333 -1.371429 -0.281690 -0.809326   \n",
      "8501  0.0  0.0  0.0  0.0  0.0  1.166667 -1.828571 -0.281690 -0.921244   \n",
      "8502  0.0  1.0  0.0  0.0  0.0  0.000000  0.285714  0.300469  0.394128   \n",
      "8503  0.0  0.0  0.0  0.0  0.0 -0.500000  0.171429 -0.018779 -0.239378   \n",
      "8504  0.0  0.0  0.0  0.0  0.0 -0.833333  0.200000  0.206573  0.191019   \n",
      "8505  0.0  0.0  0.0  0.0  0.0 -0.666667 -1.742857 -0.281690 -0.932297   \n",
      "8506  0.0  1.0  0.0  0.0  0.0 -0.333333  0.371429  0.281690  0.544041   \n",
      "8507  0.0  1.0  0.0  0.0  0.0  0.333333 -0.971429 -0.244131 -0.678066   \n",
      "8508  0.0  0.0  0.0  0.0  0.0  0.166667  0.257143  0.356808  0.398964   \n",
      "8509  0.0  0.0  0.0  0.0  0.0  2.166667  0.228571 -0.281690 -0.001036   \n",
      "8510  0.0  0.0  0.0  0.0  0.0 -0.500000  0.285714  0.056338  0.001036   \n",
      "8511  0.0  0.0  0.0  0.0  0.0 -0.500000  0.257143  0.018779  0.405872   \n",
      "8512  0.0  1.0  0.0  0.0  0.0 -0.500000 -1.228571 -0.281690 -0.879793   \n",
      "8513  0.0  0.0  0.0  0.0  0.0 -0.333333  0.457143  0.075117  0.711917   \n",
      "8514  0.0  0.0  0.0  0.0  0.0 -0.333333  0.028571  0.976526  0.591019   \n",
      "\n",
      "            19        20        21        22        23        24        25  \\\n",
      "0     1.134615 -0.050847  0.103896 -0.225989  0.490566  0.515464  0.465753   \n",
      "1    -0.019231 -0.220339  2.285714 -0.971751 -0.188679  0.618557  0.246575   \n",
      "2    -0.807692  0.322034 -0.402597  0.231638  0.943396  1.072165  1.000000   \n",
      "3    -0.326923 -0.474576 -0.233766 -0.559322  0.245283 -0.051546 -0.013699   \n",
      "4     0.923077 -0.016949 -0.350649 -0.807910 -0.830189 -0.628866 -0.794521   \n",
      "5    -0.346154  0.000000 -0.207792 -0.723164 -0.943396 -0.505155 -0.739726   \n",
      "6     2.596154  1.627119 -0.064935  1.322034  0.490566  1.185567  0.958904   \n",
      "7     0.038462  0.271186 -0.220779  0.446328  0.320755  0.762887  0.561644   \n",
      "8     0.134615 -0.220339  0.519481  0.124294 -0.113208  0.195876  0.000000   \n",
      "9    -0.134615  0.000000  0.610390  0.231638  0.509434  0.597938  0.479452   \n",
      "10    0.326923  1.322034 -0.168831  0.627119  1.792453  0.948454  1.219178   \n",
      "11   -1.153846 -1.508475 -0.402597 -0.468927  1.584906  0.278351  0.684932   \n",
      "12   -0.403846  0.389831 -0.181818  0.265537 -0.245283 -0.195876 -0.287671   \n",
      "13    0.384615  0.355932 -0.285714 -0.118644  1.094340  0.814433  0.863014   \n",
      "14    0.846154  0.593220 -0.350649 -0.248588  0.075472  0.329897  0.178082   \n",
      "15    0.115385 -0.169492  1.168831 -0.429379 -0.169811 -0.278351 -0.301370   \n",
      "16   -0.326923 -0.203390  4.844156 -1.045198 -0.547170 -0.536082 -0.616438   \n",
      "17    0.692308  0.406780 -0.402597  0.909605  1.698113  0.319588  0.753425   \n",
      "18    0.173077  0.067797 -0.181818 -0.406780  0.075472  0.340206  0.191781   \n",
      "19    0.115385  1.135593 -0.389610  1.209040  0.547170  0.938144  0.739726   \n",
      "20    0.423077 -0.101695  0.753247  0.276836 -0.641509 -0.587629 -0.698630   \n",
      "21    0.384615  2.135593 -0.402597  1.220339  0.698113  1.000000  0.890411   \n",
      "22    0.346154  1.457627  0.506494  2.242938 -0.207547 -0.443299 -0.424658   \n",
      "23   -1.000000 -1.372881 -0.363636  0.186441  0.754717  0.711340  0.643836   \n",
      "24    1.057692  1.457627 -0.402597  0.175141  1.113208  1.134021  1.123288   \n",
      "25   -0.096154 -0.186441  3.285714 -0.401130 -0.962264 -0.247423 -0.602740   \n",
      "26    1.750000  0.932203  0.051948  1.067797 -0.924528 -0.422680 -0.657534   \n",
      "27   -1.480769 -0.881356 -0.402597  1.779661  0.320755  1.185567  0.890411   \n",
      "28    0.442308 -0.084746  0.701299 -0.265537 -0.811321 -0.298969 -0.534247   \n",
      "29   -0.673077 -0.322034  2.337662 -0.960452 -1.056604 -0.556701 -0.808219   \n",
      "...        ...       ...       ...       ...       ...       ...       ...   \n",
      "8485  0.869565  0.603509 -0.672474  0.677116  0.447368  0.674772  0.603774   \n",
      "8486  1.169960  0.505263  0.017422  0.438871  0.377193 -0.079027  0.090566   \n",
      "8487  0.252964 -1.459649  0.174216 -1.510972  1.850877  0.018237  0.769811   \n",
      "8488 -0.458498  0.252632  1.390244 -0.608150 -0.307018 -0.832827 -0.679245   \n",
      "8489 -0.916996 -1.291228  0.386760 -0.557994 -0.254386  0.091185 -0.105660   \n",
      "8490 -0.332016 -0.828070 -0.006969  0.388715 -0.236842 -0.285714 -0.301887   \n",
      "8491  0.110672 -0.533333 -0.759582 -0.407524 -0.289474 -0.358663 -0.377358   \n",
      "8492 -0.521739 -0.266667  1.801394  1.460815 -0.342105 -0.382979 -0.437736   \n",
      "8493 -0.031621  0.000000  0.303136 -0.739812 -0.271930 -0.151976 -0.211321   \n",
      "8494 -0.047431  0.547368  0.031359  1.786834 -0.254386 -0.395137 -0.377358   \n",
      "8495  2.782609  0.168421 -0.087108  1.203762  0.307018  1.805471  1.222642   \n",
      "8496  0.869565 -0.729825 -0.790941 -0.257053  1.342105  1.635258  1.524528   \n",
      "8497  1.628458  1.964912 -1.125436  3.272727  1.745614  1.890578  1.932075   \n",
      "8498  2.308300  1.529825 -0.073171  0.833856 -0.254386  1.173252  0.694340   \n",
      "8499 -0.110672  3.607018 -1.132404  2.068966  0.096491  0.346505  0.226415   \n",
      "8500 -1.122530 -2.554386  0.463415 -0.658307  0.359649  0.127660  0.211321   \n",
      "8501 -0.837945  3.242105  0.027875  2.670846 -0.640351 -1.696049 -1.358491   \n",
      "8502 -0.632411 -0.084211  0.728223 -0.357367 -0.236842 -0.492401 -0.422642   \n",
      "8503 -0.079051 -0.238596 -0.466899  0.200627  0.008772  0.613982  0.377358   \n",
      "8504 -0.869565 -0.771930 -0.111498 -0.119122 -0.271930 -0.516717 -0.467925   \n",
      "8505  2.814229  2.961404 -0.261324  7.893417  1.675439 -0.115502  0.618868   \n",
      "8506  0.347826  0.364912  0.459930 -0.037618 -0.324561 -0.541033 -0.498113   \n",
      "8507 -1.122530 -0.785965  0.421603 -0.225705 -0.236842 -0.297872 -0.316981   \n",
      "8508  0.189723  0.126316 -0.229965 -0.413793  0.324561  0.431611  0.347170   \n",
      "8509 -0.632411  0.814035  1.320557 -0.764890 -0.412281 -0.759878 -0.679245   \n",
      "8510  0.158103  0.140351  0.059233  0.457680 -0.254386 -0.273556 -0.316981   \n",
      "8511  0.000000 -0.561404  0.404181 -0.288401 -0.078947  0.224924  0.075472   \n",
      "8512 -2.039526 -3.607018  0.027875 -1.159875 -0.219298  0.480243  0.196226   \n",
      "8513  0.015810 -0.224561  0.024390 -0.526646 -0.254386 -0.565350 -0.513208   \n",
      "8514  0.094862 -0.112281  0.184669 -0.476489 -0.307018 -0.164134 -0.271698   \n",
      "\n",
      "            26     27        28        29  \n",
      "0     0.196721  0.400  0.142857 -0.618182  \n",
      "1     0.204918  0.000  1.428571 -0.109091  \n",
      "2    -0.631148  0.100  1.642857  2.309091  \n",
      "3    -0.090164 -0.100  0.285714 -0.454545  \n",
      "4     0.426230 -0.600 -0.571429 -1.290909  \n",
      "5     0.836066  0.300 -0.428571  0.436364  \n",
      "6     0.221311  0.400  0.071429 -0.163636  \n",
      "7    -0.377049 -0.500  1.285714 -0.290909  \n",
      "8     1.819672  1.500  0.071429  2.381818  \n",
      "9    -0.336066  0.100  0.500000 -0.945455  \n",
      "10   -0.516393 -0.400  0.500000 -0.127273  \n",
      "11   -0.344262 -0.800 -0.285714  0.200000  \n",
      "12   -0.098361 -0.300 -0.142857  0.090909  \n",
      "13   -0.557377 -0.100  0.285714 -0.054545  \n",
      "14   -0.352459 -0.600  0.142857 -0.890909  \n",
      "15    0.655738  0.300 -0.285714  0.109091  \n",
      "16    0.278689  0.600 -0.428571 -0.672727  \n",
      "17   -0.467213 -0.600 -0.071429 -1.054545  \n",
      "18    0.032787  0.100  0.428571  0.254545  \n",
      "19   -0.409836 -0.300 -0.071429  0.254545  \n",
      "20    1.311475 -0.300 -0.428571  0.036364  \n",
      "21   -0.655738 -0.800  2.428571  0.400000  \n",
      "22   -0.040984  1.000 -0.500000  0.327273  \n",
      "23   -0.573770 -0.700 -0.142857  0.236364  \n",
      "24   -0.352459 -0.700  0.571429  0.090909  \n",
      "25    1.663934 -0.400 -0.500000  0.163636  \n",
      "26    2.778689  0.100 -0.428571  0.309091  \n",
      "27   -0.590164 -0.900  1.285714  1.436364  \n",
      "28    2.122951  1.300 -0.500000  0.927273  \n",
      "29    0.950820  0.300 -0.500000  0.890909  \n",
      "...        ...    ...       ...       ...  \n",
      "8485 -0.331776  0.000  2.666667 -0.513761  \n",
      "8486  0.434579 -0.875  1.666667 -0.036697  \n",
      "8487  0.462617  1.250  0.555556 -0.220183  \n",
      "8488 -0.434579  0.250 -0.333333 -0.733945  \n",
      "8489 -0.051402  0.000  0.000000  0.330275  \n",
      "8490  1.088785 -0.125 -0.277778  0.770642  \n",
      "8491  2.556075  2.000 -0.444444  1.853211  \n",
      "8492 -0.948598 -0.875 -0.666667 -1.486239  \n",
      "8493  0.228972 -0.625 -0.222222 -0.513761  \n",
      "8494  0.948598  0.125  0.055556  0.513761  \n",
      "8495  4.406542  1.000 -0.166667  0.660550  \n",
      "8496  0.313084 -0.500 -0.388889 -0.678899  \n",
      "8497 -0.415888 -0.750  2.888889  0.513761  \n",
      "8498  1.210280  0.000  1.444444 -0.348624  \n",
      "8499 -0.219626  1.625  0.222222  3.486239  \n",
      "8500  0.219626 -0.625  0.055556 -0.183486  \n",
      "8501  0.948598  1.875 -0.666667  5.743119  \n",
      "8502 -0.219626  0.125 -0.055556  0.110092  \n",
      "8503  0.210280  1.250  0.500000  0.366972  \n",
      "8504  0.705607 -0.375  0.277778  1.467890  \n",
      "8505  1.686916 -1.875 -0.666667 -2.256881  \n",
      "8506  0.387850 -0.250 -0.333333 -0.513761  \n",
      "8507 -0.649533 -0.750 -0.111111 -0.275229  \n",
      "8508 -0.144860 -0.875 -0.055556 -0.660550  \n",
      "8509 -0.023364  0.250  0.055556  0.000000  \n",
      "8510 -0.079439  0.500 -0.166667 -0.275229  \n",
      "8511  0.313084 -0.375  0.166667 -0.788991  \n",
      "8512 -0.247664  0.125 -0.666667 -0.073394  \n",
      "8513  0.397196  0.375 -0.500000  0.036697  \n",
      "8514  0.219626  0.250 -0.222222 -0.311927  \n",
      "\n",
      "[8515 rows x 20 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler\n",
    "n_jobs = -1 # All processors\n",
    "\n",
    "scaler = RobustScaler\n",
    "\n",
    "col_transformer = construct_transformer(X_train, scaler, 'Season End', n_jobs)\n",
    "if debug:\n",
    "    X_train_transformed = col_transformer.fit_transform(X_train)\n",
    "    print(\"Transformed shape:\", X_train_transformed.shape)\n",
    "    print(pd.DataFrame(X_train_transformed).iloc[:, 10:30])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.feature_selection import SelectFromModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_train_transformed = col_transformer.fit_transform(X_train)\n",
    "#estimator = LinearSVC()\n",
    "#feature_selector_cv = feature_selection.RFECV(estimator, cv=5, step=1, scoring=\"neg_mean_squared_error\")\n",
    "#feature_selector_cv.fit(X_train_transformed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ignore for now\n",
    "#clf = Pipeline([\n",
    "#  ('transform', col_transformer),\n",
    "#  ('feature_selection', SelectFromModel(LinearSVC(max_iter = 4000))),\n",
    "#  ('classification', RandomForestClassifier())\n",
    "#])\n",
    "#clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('transform',\n",
       "                 ColumnTransformer(n_jobs=-1, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('cat',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('ohe',\n",
       "                                                                   OneHotEncoder(categorical_features=None,\n",
       "                                                                                 categories=None,\n",
       "                                                                                 drop=None,\n",
       "                                                                                 dtype=<class 'numpy.float64'>,\n",
       "                                                                                 handle_unknown='ignore',\n",
       "                                                                                 n_values=None,\n",
       "                                                                                 sparse=False))],\n",
       "                                                           verbose=False...\n",
       "                ('rfc',\n",
       "                 RandomForestClassifier(bootstrap=True, class_weight=None,\n",
       "                                        criterion='gini', max_depth=None,\n",
       "                                        max_features=10, max_leaf_nodes=None,\n",
       "                                        min_impurity_decrease=0.0,\n",
       "                                        min_impurity_split=None,\n",
       "                                        min_samples_leaf=2, min_samples_split=3,\n",
       "                                        min_weight_fraction_leaf=0.0,\n",
       "                                        n_estimators=180, n_jobs=-1,\n",
       "                                        oob_score=True, random_state=None,\n",
       "                                        verbose=0, warm_start=True))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_pipe = Pipeline([('transform', col_transformer), ('rfc', \n",
    "RandomForestClassifier(n_jobs=n_jobs,\n",
    "                       min_samples_split=3,\n",
    "                       warm_start = True,\n",
    "                       max_features=10,\n",
    "                       n_estimators = 180,\n",
    "                       min_samples_leaf=2,\n",
    "                       oob_score=True\n",
    "                      ))])\n",
    "#ml_pipe = Pipeline([('transform', col_transformer), ('svc', LinearSVC())])\n",
    "#ml_pipe = Pipeline([('transform', col_transformer), ('knn', KNeighborsClassifier(n_jobs=n_jobs))])\n",
    "#ml_pipe = Pipeline([('transform', col_transformer), ('mlp', MLPClassifier(activation='tanh'))])\n",
    "#ml_pipe = Pipeline([('transform', col_transformer), ('log', LogisticRegression(max_iter=4000))])\n",
    "\n",
    "ml_pipe.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9694656488549618"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_pipe.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7059652418976045"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_pipe.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
